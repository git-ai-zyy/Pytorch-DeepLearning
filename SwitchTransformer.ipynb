{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/202502162115032.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "\n",
    "    hidden_dim = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    ff_dim = 2048\n",
    "    # MoE\n",
    "    num_experts: int = 4\n",
    "    capacity_factor = 1.0\n",
    "    use_aux_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, config: Config, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.num_experts\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.capacity_factor = config.capacity_factor\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.w_gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "\n",
    "    def forward(self, x, use_aux_loss=False):\n",
    "        # x: (B, S, H )\n",
    "\n",
    "        # Get the probability of each expert\n",
    "        gate_scores = F.softmax(self.w_gate(x), dim=-1)  # (B, S, E)\n",
    "\n",
    "        # Determine the top-1 expert for each token\n",
    "        capacity = int(self.capacity_factor * self.num_experts)\n",
    "        top_k_scores, top_k_indices = torch.topk(gate_scores, capacity, dim=-1)\n",
    "\n",
    "        # Mask for enforce sparsity\n",
    "        mask = torch.zeros_like(gate_scores).scatter_(1, top_k_indices, 1.0)\n",
    "\n",
    "        masked_gate_scores = gate_scores * mask\n",
    "\n",
    "        # Denominators\n",
    "        denominators = masked_gate_scores.sum(dim=0, keepdim=True) + self.epsilon\n",
    "\n",
    "        gate_scores = (masked_gate_scores / denominators) * capacity\n",
    "\n",
    "        if use_aux_loss:\n",
    "            load = gate_scores.sum(0)\n",
    "            importance = gate_scores.sum(1)\n",
    "\n",
    "            loss = ((load - importance) ** 2).mean()\n",
    "\n",
    "            return gate_scores, loss\n",
    "\n",
    "        return gate_scores, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.ff_dim = config.ff_dim\n",
    "\n",
    "        self.linear1 = nn.Linear(self.hidden_dim, self.ff_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.ff_dim, self.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.activation(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchMoE(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = config.hidden_dim\n",
    "        self.ff_dim = config.ff_dim\n",
    "        self.num_experts = config.num_experts\n",
    "        self.capacity_factor = config.capacity_factor\n",
    "        self.mult = 4\n",
    "        self.use_aux_loss = config.use_aux_loss\n",
    "\n",
    "        self.experts = nn.ModuleList(\n",
    "            [FeedForward(config) for _ in range(self.num_experts)]\n",
    "        )\n",
    "\n",
    "        self.router = Router(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_scores, loss = self.router(x, self.use_aux_loss)\n",
    "\n",
    "        # Dispatch to experts\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "\n",
    "        if torch.isnan(gate_scores).any():\n",
    "            print(\"nan in gate_scores\")\n",
    "            gate_scores[torch.isnan(gate_scores)] = 0\n",
    "\n",
    "        # Stack and weight outputs\n",
    "        stacked_expert_outputs = torch.stack(expert_outputs, dim=-1)\n",
    "\n",
    "        if torch.isnan(stacked_expert_outputs).any():\n",
    "            stacked_expert_outputs[torch.isnan(stacked_expert_outputs)] = 0\n",
    "\n",
    "        moe_output = torch.sum(\n",
    "            gate_scores.unsqueeze(-2) * stacked_expert_outputs, dim=-1\n",
    "        )\n",
    "\n",
    "        return moe_output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "\n",
    "        assert (\n",
    "            self.hidden_dim % self.num_heads == 0\n",
    "        ), \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.head_dim = self.hidden_dim // self.num_heads\n",
    "\n",
    "        self.qkv_ln = nn.Linear(self.hidden_dim, 3 * self.hidden_dim)\n",
    "        self.out_ln = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        q, k, v = map(\n",
    "            lambda t: t.view(\n",
    "                x.size(0), x.size(1), self.num_heads, self.head_dim\n",
    "            ).transpose(1, 2),\n",
    "            self.qkv_ln(x).chunk(3, dim=-1),\n",
    "        )\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim**0.5)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        output = (\n",
    "            torch.matmul(scores, v)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(x.size(0), x.size(1), self.hidden_dim)\n",
    "        )\n",
    "\n",
    "        return self.out_ln(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchMoEBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MHA(config)\n",
    "        self.moe = SwitchMoE(config)\n",
    "        self.ff = FeedForward(config)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.norm1(x), mask)\n",
    "        x, aux_loss = self.moe(self.norm2(x))\n",
    "        x = x + self.ff(x)\n",
    "\n",
    "        return x, aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeEmbedding(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(1000, config.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchTransformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = FakeEmbedding(config)\n",
    "\n",
    "        self.num_layers = config.num_layers\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [SwitchMoEBlock(config) for _ in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        aux_losses = []\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x, aux_loss = block(x, mask)\n",
    "            if aux_loss is not None:\n",
    "                aux_losses.append(aux_loss)\n",
    "\n",
    "        return x, aux_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 1000, (32, 128))\n",
    "tgt_mask = causal_mask(128)\n",
    "\n",
    "config = Config()\n",
    "transformer = SwitchTransformer(config)\n",
    "\n",
    "assert transformer(x, tgt_mask)[0].shape == (32, 128, 512)\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
