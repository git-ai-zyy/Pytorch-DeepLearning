{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Image Embedding Config\n",
    "    image_size = 256\n",
    "    patch_size = 16\n",
    "    num_channels = 3\n",
    "    hidden_size = 768\n",
    "\n",
    "    hidden_dropout = 0.1\n",
    "\n",
    "    num_layers = 12\n",
    "    num_heads = 8\n",
    "    num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    # Patch image and Linear projection\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.num_channels = config.num_channels\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        assert (\n",
    "            self.image_size % self.patch_size == 0\n",
    "        ), \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        self.patch_and_projection = nn.Conv2d(\n",
    "            self.num_channels,\n",
    "            self.hidden_size,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, C, H, W) -> (B, hidden_size, H // patch_size, W // patch_size)\n",
    "        x = self.patch_and_projection(x)\n",
    "        return x.flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1,\n",
    "                (config.image_size // config.patch_size) ** 2 + 1,\n",
    "                config.hidden_size,\n",
    "            )\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(config)\n",
    "        self.position_embedding = PositionEmbedding(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = self.position_embedding(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        assert (\n",
    "            config.hidden_size % self.num_heads == 0\n",
    "        ), f\"Hidden size ({config.hidden_size}) must be divisible by the number of heads ({self.num_heads})\"\n",
    "\n",
    "        self.attention_head_size = config.hidden_size // config.num_heads\n",
    "\n",
    "        self.qkv_linear = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n",
    "\n",
    "        self.out_linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = map(\n",
    "            lambda t: t.view(\n",
    "                t.shape[0], t.shape[1], self.num_heads, self.attention_head_size\n",
    "            ).transpose(1, 2),\n",
    "            self.qkv_linear(x).chunk(3, dim=-1),\n",
    "        )\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.attention_head_size**0.5)\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        attention = torch.matmul(self.dropout(scores), v)\n",
    "\n",
    "        attention = (\n",
    "            attention.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(x.shape[0], x.shape[1], self.num_heads * self.attention_head_size)\n",
    "        )\n",
    "\n",
    "        return self.dropout(self.out_linear(attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(0.7978845608 * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.Linear(config.hidden_size, config.hidden_size * 4)\n",
    "        self.act = GELU()\n",
    "        self.ln2 = nn.Linear(config.hidden_size * 4, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.ln2(self.act(self.ln1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X: (batch_size, seq_len, features)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MLP(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.norm(x)))\n",
    "        x = x + self.dropout(self.mlp(self.norm(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [EncoderBlock(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProjector(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_classes = config.num_classes\n",
    "\n",
    "        self.ln = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(x[:, 0])  # Just the CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_embedding = ImageEmbedding(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.mlp_head = MLPProjector(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.constant_(module.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "config = Config()\n",
    "model = ViT(config)\n",
    "model.apply(init_weight)\n",
    "\n",
    "x = torch.randn(8, 3, 256, 256)\n",
    "output = model(x)\n",
    "\n",
    "assert output.shape == (\n",
    "    8,\n",
    "    config.num_classes,\n",
    "), f\"Output shape is not as expected: {output.shape}\"\n",
    "\n",
    "print(\"Test passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
